{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBjaVH1qOt3mo5K3dLEC5S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaabhijith13/linkedIN_posts/blob/main/Spark_rdd's.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This simple word count example demonstrates Spark’s core strengths: lazy evaluation, DAG-based execution, in-memory processing, and fault tolerance through lineage — all without writing intermediate data to disk.**\n",
        "\n",
        "Author: Abhijith\n",
        "\n",
        "Date: 1/6/2026\n",
        "\n",
        "Happy New year!"
      ],
      "metadata": {
        "id": "fVIkMJXRJXpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Pyspark\n"
      ],
      "metadata": {
        "id": "YLABn8FWGNOf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWw-xA0k_N1L",
        "outputId": "22effdae-6ae2-452a-d15f-53bcb390c992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Pyspark And Context"
      ],
      "metadata": {
        "id": "EkubmZIGGUzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf() \\\n",
        "    .setAppName(\"RDD-Deep-Dive\") \\\n",
        "    .setMaster(\"local[*]\")\n",
        "\n",
        "sc = SparkContext.getOrCreate(conf)\n"
      ],
      "metadata": {
        "id": "qFu4xJP8_S3v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating RDD's\n"
      ],
      "metadata": {
        "id": "15hDxcNz_oR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\"Spark is a fast and general-purpose cluster computing system\n",
        "Spark provides high-level APIs in Java Python Scala and R\n",
        "Spark is optimized for large-scale data processing\n",
        "Hadoop MapReduce is a disk-based processing framework\n",
        "Spark improves performance by keeping data in memory\n",
        "ERROR Failed to connect to data source\n",
        "Spark supports batch processing streaming and machine learning\n",
        "ERROR Timeout occurred while reading data\n",
        "HDFS stores data reliably across distributed nodes\n",
        "Spark runs on top of YARN Mesos or its own standalone cluster manager\n",
        "ERROR Disk read failure on worker node\n",
        "Big data processing requires fault tolerance and scalability\n",
        "Spark uses lazy evaluation to optimize execution plans\n",
        "MapReduce processes data in multiple stages\n",
        "ERROR Network failure during shuffle phase\n",
        "\"\"\"\n",
        "with open('spark_file.txt', 'w') as file:\n",
        "    file.write(data)"
      ],
      "metadata": {
        "id": "k5NCN_-f_qK7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.textFile(\"spark_file.txt\")\n",
        "rdd.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_hplxhRBJ6H",
        "outputId": "3ec1ebe7-50f3-4d09-e40f-191599815db1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Spark is a fast and general-purpose cluster computing system',\n",
              " 'Spark provides high-level APIs in Java Python Scala and R',\n",
              " 'Spark is optimized for large-scale data processing',\n",
              " 'Hadoop MapReduce is a disk-based processing framework',\n",
              " 'Spark improves performance by keeping data in memory']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numbers = sc.parallelize([1, 2, 3, 4, 5, 9, 67, 97, 2122, 3, 4, 4, 5, 2, 5,])\n",
        "numbers.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSRngvKoBREJ",
        "outputId": "0359d264-8802-46c6-b4c8-cdfd0ef31525"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 9, 67, 97, 2122, 3, 4, 4, 5, 2, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numbers.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKYCaczvBYXc",
        "outputId": "4fc1d00b-f410-4267-9f06-f23c2b1199f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numbers = numbers.repartition(4)\n",
        "numbers.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZylMu6ZnBb6i",
        "outputId": "6bd6c07c-5cf0-411c-a8eb-03b362519d78"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformations (Lazy Operations)\n",
        "\n",
        "Transformations do not trigger execution."
      ],
      "metadata": {
        "id": "ZqEEh9StGptP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapped = rdd.map(lambda line: line.upper())\n",
        "filtered = mapped.filter(lambda line: \"ERROR\" in line)\n",
        "#No execution yet"
      ],
      "metadata": {
        "id": "QVmOU3XnB9qZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered.count() #now exectued"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb-v9ck1CUxY",
        "outputId": "61adc59c-35b3-4903-ba7c-364051b98fee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered.toDebugString() #string representation of the RDD lineage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS5H1ltFDUPv",
        "outputId": "f50cec36-0096-4576-bc8e-e104e0ad431b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'(2) PythonRDD[10] at RDD at PythonRDD.scala:56 []\\n |  spark_file.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\\n |  spark_file.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Lineage\n",
        "1.   PythonRDD[18] at RDD at PythonRDD.scala:56 []\\n\n",
        "2.   spark_file.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0\n",
        "3. spark_file.txt HadoopRDD[2] at textFile at NativeMethodAccessorImpl.java:0\n",
        "\n"
      ],
      "metadata": {
        "id": "qIgZyX5eG0Fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Caching RDDs (Performance Optimization)"
      ],
      "metadata": {
        "id": "wRNjoDjlHWd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapped = rdd.map(lambda line: line.upper())\n",
        "\n",
        "errors = rdd.filter(lambda line: \"ERROR\" in line)\n",
        "\n",
        "errors.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZbGd_BgDpsj",
        "outputId": "ce0ff593-d66b-4a03-fcb1-3727d200f629"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[11] at RDD at PythonRDD.scala:56"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "errors.count(), errors.take(10) #count of the number of error messages in the file above"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1mp16W5D4A4",
        "outputId": "74797a68-d09a-4da8-f736-c0877a6898a6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,\n",
              " ['ERROR Failed to connect to data source',\n",
              "  'ERROR Timeout occurred while reading data',\n",
              "  'ERROR Disk read failure on worker node',\n",
              "  'ERROR Network failure during shuffle phase'])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Cached in executor memory\n",
        "2. Avoids recomputation\n",
        "3. Programmer-controlled\n",
        "\n"
      ],
      "metadata": {
        "id": "pfa0A4qkHyDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_example = sc.textFile(\"spark_file.txt\")\n",
        "\n",
        "word_count = (\n",
        "    full_example.flatMap(lambda line: line.split())\n",
        "       .map(lambda word: (word.lower(), 1))\n",
        "       .reduceByKey(lambda a, b: a + b)\n",
        ")\n",
        "\n",
        "word_count.saveAsTextFile(\"finalwordcount\")"
      ],
      "metadata": {
        "id": "sVvUdHnCD594"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_count.collect() #Use only when results fit in driver memory. Spark builds a DAG and executes the entire pipeline."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvFtuJ8UETqz",
        "outputId": "9bb2411f-495f-4df5-e6fc-0ac91fb6efc9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fast', 1),\n",
              " ('and', 4),\n",
              " ('computing', 1),\n",
              " ('java', 1),\n",
              " ('python', 1),\n",
              " ('optimized', 1),\n",
              " ('for', 1),\n",
              " ('hadoop', 1),\n",
              " ('framework', 1),\n",
              " ('performance', 1),\n",
              " ('by', 1),\n",
              " ('memory', 1),\n",
              " ('error', 4),\n",
              " ('failed', 1),\n",
              " ('to', 3),\n",
              " ('supports', 1),\n",
              " ('batch', 1),\n",
              " ('streaming', 1),\n",
              " ('machine', 1),\n",
              " ('learning', 1),\n",
              " ('occurred', 1),\n",
              " ('while', 1),\n",
              " ('reading', 1),\n",
              " ('stores', 1),\n",
              " ('distributed', 1),\n",
              " ('runs', 1),\n",
              " ('of', 1),\n",
              " ('mesos', 1),\n",
              " ('own', 1),\n",
              " ('disk', 1),\n",
              " ('read', 1),\n",
              " ('failure', 2),\n",
              " ('big', 1),\n",
              " ('requires', 1),\n",
              " ('fault', 1),\n",
              " ('tolerance', 1),\n",
              " ('uses', 1),\n",
              " ('lazy', 1),\n",
              " ('optimize', 1),\n",
              " ('processes', 1),\n",
              " ('multiple', 1),\n",
              " ('network', 1),\n",
              " ('shuffle', 1),\n",
              " ('phase', 1),\n",
              " ('spark', 7),\n",
              " ('is', 3),\n",
              " ('a', 2),\n",
              " ('general-purpose', 1),\n",
              " ('cluster', 2),\n",
              " ('system', 1),\n",
              " ('provides', 1),\n",
              " ('high-level', 1),\n",
              " ('apis', 1),\n",
              " ('in', 3),\n",
              " ('scala', 1),\n",
              " ('r', 1),\n",
              " ('large-scale', 1),\n",
              " ('data', 7),\n",
              " ('processing', 4),\n",
              " ('mapreduce', 2),\n",
              " ('disk-based', 1),\n",
              " ('improves', 1),\n",
              " ('keeping', 1),\n",
              " ('connect', 1),\n",
              " ('source', 1),\n",
              " ('timeout', 1),\n",
              " ('hdfs', 1),\n",
              " ('reliably', 1),\n",
              " ('across', 1),\n",
              " ('nodes', 1),\n",
              " ('on', 2),\n",
              " ('top', 1),\n",
              " ('yarn', 1),\n",
              " ('or', 1),\n",
              " ('its', 1),\n",
              " ('standalone', 1),\n",
              " ('manager', 1),\n",
              " ('worker', 1),\n",
              " ('node', 1),\n",
              " ('scalability', 1),\n",
              " ('evaluation', 1),\n",
              " ('execution', 1),\n",
              " ('plans', 1),\n",
              " ('stages', 1),\n",
              " ('during', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_count.count() #total number of items"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqEtqsBqIYoD",
        "outputId": "d6342fcc-3fd2-425e-fb67-fdfb2bf12b9d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_count.toDebugString())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8_VSt2wFO17",
        "outputId": "25e27b64-eee1-477e-e33c-2a56b9948f29"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'(2) PythonRDD[24] at collect at /tmp/ipython-input-879584768.py:1 []\\n |  MapPartitionsRDD[20] at mapPartitions at PythonRDD.scala:168 []\\n |  ShuffledRDD[19] at partitionBy at NativeMethodAccessorImpl.java:0 []\\n +-(2) PairwiseRDD[18] at reduceByKey at /tmp/ipython-input-2954201668.py:6 []\\n    |  PythonRDD[17] at reduceByKey at /tmp/ipython-input-2954201668.py:6 []\\n    |  spark_file.txt MapPartitionsRDD[16] at textFile at NativeMethodAccessorImpl.java:0 []\\n    |  spark_file.txt HadoopRDD[15] at textFile at NativeMethodAccessorImpl.java:0 []'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_count.unpersist() #remove a cached or persisted DataFrame, Dataset, or RDD from memory and/or disk storage\n",
        "#Essentially Delete all 85 items."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh9SnNftFzad",
        "outputId": "f071a794-e458-44b9-905f-d00a51dd9219"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[24] at collect at /tmp/ipython-input-879584768.py:1"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fault Tolerance via Lineage\n",
        "\n",
        "If a partition of anystep is lost:\n",
        "\n",
        "Spark replays:\n",
        "\n",
        "flatmap → reduce → filtreduceByKey\n",
        "\n",
        "Look below once we .collect() Spark goes through the process again, demonstrating Fault Tolerance"
      ],
      "metadata": {
        "id": "jsjDymSbIhsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_count.collect() #Use only when results fit in driver memory."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWYCjun2F4QY",
        "outputId": "2e3d2aa2-89d3-4906-b134-0aaadcf620fb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fast', 1),\n",
              " ('and', 4),\n",
              " ('computing', 1),\n",
              " ('java', 1),\n",
              " ('python', 1),\n",
              " ('optimized', 1),\n",
              " ('for', 1),\n",
              " ('hadoop', 1),\n",
              " ('framework', 1),\n",
              " ('performance', 1),\n",
              " ('by', 1),\n",
              " ('memory', 1),\n",
              " ('error', 4),\n",
              " ('failed', 1),\n",
              " ('to', 3),\n",
              " ('supports', 1),\n",
              " ('batch', 1),\n",
              " ('streaming', 1),\n",
              " ('machine', 1),\n",
              " ('learning', 1),\n",
              " ('occurred', 1),\n",
              " ('while', 1),\n",
              " ('reading', 1),\n",
              " ('stores', 1),\n",
              " ('distributed', 1),\n",
              " ('runs', 1),\n",
              " ('of', 1),\n",
              " ('mesos', 1),\n",
              " ('own', 1),\n",
              " ('disk', 1),\n",
              " ('read', 1),\n",
              " ('failure', 2),\n",
              " ('big', 1),\n",
              " ('requires', 1),\n",
              " ('fault', 1),\n",
              " ('tolerance', 1),\n",
              " ('uses', 1),\n",
              " ('lazy', 1),\n",
              " ('optimize', 1),\n",
              " ('processes', 1),\n",
              " ('multiple', 1),\n",
              " ('network', 1),\n",
              " ('shuffle', 1),\n",
              " ('phase', 1),\n",
              " ('spark', 7),\n",
              " ('is', 3),\n",
              " ('a', 2),\n",
              " ('general-purpose', 1),\n",
              " ('cluster', 2),\n",
              " ('system', 1),\n",
              " ('provides', 1),\n",
              " ('high-level', 1),\n",
              " ('apis', 1),\n",
              " ('in', 3),\n",
              " ('scala', 1),\n",
              " ('r', 1),\n",
              " ('large-scale', 1),\n",
              " ('data', 7),\n",
              " ('processing', 4),\n",
              " ('mapreduce', 2),\n",
              " ('disk-based', 1),\n",
              " ('improves', 1),\n",
              " ('keeping', 1),\n",
              " ('connect', 1),\n",
              " ('source', 1),\n",
              " ('timeout', 1),\n",
              " ('hdfs', 1),\n",
              " ('reliably', 1),\n",
              " ('across', 1),\n",
              " ('nodes', 1),\n",
              " ('on', 2),\n",
              " ('top', 1),\n",
              " ('yarn', 1),\n",
              " ('or', 1),\n",
              " ('its', 1),\n",
              " ('standalone', 1),\n",
              " ('manager', 1),\n",
              " ('worker', 1),\n",
              " ('node', 1),\n",
              " ('scalability', 1),\n",
              " ('evaluation', 1),\n",
              " ('execution', 1),\n",
              " ('plans', 1),\n",
              " ('stages', 1),\n",
              " ('during', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_count.count() #Same count as above, before we removed the data."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AXGJftxIVDR",
        "outputId": "a558117d-770c-4154-9745-0360df7b7502"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CleanUp"
      ],
      "metadata": {
        "id": "fxAj0JP2EgIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "H9C2QBEQEW_q"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}