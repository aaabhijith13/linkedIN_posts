{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Live BTC/ETH streaming with Kafka (Producer \u2192 Topics \u2192 Consumer Analytics)\n",
        "\n",
        "This notebook shows an end-to-end pattern for **streaming live BTC/ETH ticks** into **Kafka**, then consuming them for:\n",
        "- **cleaning & normalization**\n",
        "- **windowed aggregations** (OHLCV-like bars, returns, spreads)\n",
        "- **basic anomaly flags** (jump detection)\n",
        "- **persistence** (optional) and **visualization**\n",
        "\n",
        "> Notes:\n",
        "> - The code is written to run locally against a Kafka broker (e.g., `localhost:9092`).\n",
        "> - \"Live\" data can come from an exchange WebSocket (e.g., Binance). If your environment blocks outbound network access, run the producer on a machine that can reach the exchange and keep the consumer local.\n",
        "> - Replace the WebSocket endpoint if you prefer Coinbase, Kraken, etc.\n",
        "\n",
        "---\n",
        "\n",
        "## Architecture\n",
        "\n",
        "**Producer (WebSocket)**  \n",
        "`Exchange WS \u2192 parse \u2192 Kafka topic: crypto.ticks`\n",
        "\n",
        "**Kafka**\n",
        "- Topic: `crypto.ticks` (keyed by symbol)\n",
        "- Optional topic: `crypto.alerts` (anomaly events)\n",
        "\n",
        "**Consumer (Analytics)**\n",
        "`Kafka \u2192 schema validate \u2192 window aggregates \u2192 visualize/store`\n",
        "\n",
        "---\n",
        "\n",
        "## Dependencies\n",
        "\n",
        "You'll use:\n",
        "- `kafka-python` for Kafka\n",
        "- `websockets` for live WS ingestion (producer)\n",
        "- `pydantic` (optional) for validation\n",
        "- `pandas`/`numpy`/`matplotlib` for analytics\n",
        "\n",
        "Install (one-time):\n",
        "```bash\n",
        "pip install kafka-python websockets pydantic pandas numpy matplotlib pytest\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kafka quickstart (Docker)\n",
        "\n",
        "If you don't already have Kafka running, the quickest local setup is Docker. Save this as `docker-compose.yml` and run `docker compose up -d`.\n",
        "\n",
        "```yaml\n",
        "services:\n",
        "  zookeeper:\n",
        "    image: confluentinc/cp-zookeeper:7.6.1\n",
        "    environment:\n",
        "      ZOOKEEPER_CLIENT_PORT: 2181\n",
        "      ZOOKEEPER_TICK_TIME: 2000\n",
        "\n",
        "  kafka:\n",
        "    image: confluentinc/cp-kafka:7.6.1\n",
        "    depends_on:\n",
        "      - zookeeper\n",
        "    ports:\n",
        "      - \"9092:9092\"\n",
        "    environment:\n",
        "      KAFKA_BROKER_ID: 1\n",
        "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
        "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n",
        "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT\n",
        "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
        "```\n",
        "\n",
        "Create a topic:\n",
        "```bash\n",
        "docker exec -it $(docker ps -qf \"name=kafka\") kafka-topics --bootstrap-server localhost:9092 --create --topic crypto.ticks --partitions 6 --replication-factor 1\n",
        "```\n",
        "\n",
        "> Partitions: using 6 partitions lets BTC/ETH and future symbols scale across consumers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Notebook config (adjust as needed)\n",
        "\n",
        "KAFKA_BOOTSTRAP = \"localhost:9092\"\n",
        "TICKS_TOPIC = \"crypto.ticks\"\n",
        "ALERTS_TOPIC = \"crypto.alerts\"  # optional\n",
        "SYMBOLS = [\"BTCUSDT\", \"ETHUSDT\"]\n",
        "\n",
        "# Windowing for analytics\n",
        "BAR_SECONDS = 10  # make this 60 for 1-minute bars\n",
        "MAX_POINTS_TO_PLOT = 300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data model (tick schema)\n",
        "\n",
        "We'll standardize all ticks into a compact JSON record:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"source\": \"binance\",\n",
        "  \"symbol\": \"BTCUSDT\",\n",
        "  \"ts_ms\": 1710000000000,\n",
        "  \"price\": 67000.12,\n",
        "  \"qty\": 0.0031\n",
        "}\n",
        "```\n",
        "\n",
        "Why standardize?\n",
        "- Consumers shouldn't care about exchange-specific fields.\n",
        "- Enables multiple producers feeding the same topic.\n",
        "- Makes testing and validation straightforward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, Optional, List\n",
        "import json\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unit tests first (schema + transformations)\n",
        "\n",
        "Per good streaming practice, you test:\n",
        "- parsing input events \u2192 normalized tick\n",
        "- validation rejects malformed input\n",
        "- window aggregation produces correct OHLCV\n",
        "\n",
        "These tests run locally and do **not** require Kafka."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- tests ---\n",
        "\n",
        "def test_normalize_tick_accepts_valid():\n",
        "    raw = {\"symbol\": \"BTCUSDT\", \"ts_ms\": 1000, \"price\": 10.0, \"qty\": 0.5, \"source\": \"x\"}\n",
        "    out = normalize_tick(raw)\n",
        "    assert out[\"symbol\"] == \"BTCUSDT\"\n",
        "    assert out[\"ts_ms\"] == 1000\n",
        "    assert out[\"price\"] == 10.0\n",
        "    assert out[\"qty\"] == 0.5\n",
        "    assert out[\"source\"] == \"x\"\n",
        "\n",
        "def test_normalize_tick_rejects_missing_fields():\n",
        "    try:\n",
        "        normalize_tick({\"symbol\": \"BTCUSDT\", \"price\": 10.0})\n",
        "        assert False, \"Expected ValueError\"\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "def test_aggregate_ticks_to_bar():\n",
        "    ticks = [\n",
        "        {\"symbol\":\"BTCUSDT\",\"ts_ms\": 1000, \"price\": 10.0, \"qty\": 1.0, \"source\":\"x\"},\n",
        "        {\"symbol\":\"BTCUSDT\",\"ts_ms\": 2000, \"price\": 12.0, \"qty\": 2.0, \"source\":\"x\"},\n",
        "        {\"symbol\":\"BTCUSDT\",\"ts_ms\": 3000, \"price\": 11.0, \"qty\": 3.0, \"source\":\"x\"},\n",
        "    ]\n",
        "    bar = aggregate_to_bar(\"BTCUSDT\", window_start_ms=0, window_end_ms=10_000, ticks=ticks)\n",
        "    assert bar[\"open\"] == 10.0\n",
        "    assert bar[\"high\"] == 12.0\n",
        "    assert bar[\"low\"] == 10.0\n",
        "    assert bar[\"close\"] == 11.0\n",
        "    assert math.isclose(bar[\"volume_qty\"], 6.0)\n",
        "\n",
        "def run_tests():\n",
        "    tests = [\n",
        "        test_normalize_tick_accepts_valid,\n",
        "        test_normalize_tick_rejects_missing_fields,\n",
        "        test_aggregate_ticks_to_bar,\n",
        "    ]\n",
        "    for t in tests:\n",
        "        t()\n",
        "    print(f\"\u2705 Ran {len(tests)} tests successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation (after tests)\n",
        "\n",
        "We'll implement three core pure functions:\n",
        "1. `normalize_tick` \u2014 schema checks & coercion\n",
        "2. `aggregate_to_bar` \u2014 produce OHLCV-style bar for a fixed window\n",
        "3. `detect_jump` \u2014 simple anomaly flag based on percent-move threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def normalize_tick(obj: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    '''\n",
        "    Validate and coerce a raw tick dict into the standard schema.\n",
        "\n",
        "    Required keys:\n",
        "      - source: str\n",
        "      - symbol: str\n",
        "      - ts_ms: int (epoch milliseconds)\n",
        "      - price: float\n",
        "      - qty: float\n",
        "\n",
        "    Raises:\n",
        "      ValueError if required fields are missing or invalid.\n",
        "    '''\n",
        "    required = [\"source\", \"symbol\", \"ts_ms\", \"price\", \"qty\"]\n",
        "    for k in required:\n",
        "        if k not in obj:\n",
        "            raise ValueError(f\"Missing field: {k}\")\n",
        "\n",
        "    source = str(obj[\"source\"])\n",
        "    symbol = str(obj[\"symbol\"])\n",
        "\n",
        "    try:\n",
        "        ts_ms = int(obj[\"ts_ms\"])\n",
        "    except Exception as e:\n",
        "        raise ValueError(\"ts_ms must be int-like\") from e\n",
        "\n",
        "    try:\n",
        "        price = float(obj[\"price\"])\n",
        "        qty = float(obj[\"qty\"])\n",
        "    except Exception as e:\n",
        "        raise ValueError(\"price/qty must be numeric\") from e\n",
        "\n",
        "    if ts_ms <= 0:\n",
        "        raise ValueError(\"ts_ms must be > 0\")\n",
        "    if not (price > 0.0):\n",
        "        raise ValueError(\"price must be > 0\")\n",
        "    if qty < 0.0:\n",
        "        raise ValueError(\"qty must be >= 0\")\n",
        "\n",
        "    return {\"source\": source, \"symbol\": symbol, \"ts_ms\": ts_ms, \"price\": price, \"qty\": qty}\n",
        "\n",
        "\n",
        "def aggregate_to_bar(\n",
        "    symbol: str,\n",
        "    window_start_ms: int,\n",
        "    window_end_ms: int,\n",
        "    ticks: List[Dict[str, Any]],\n",
        ") -> Dict[str, Any]:\n",
        "    '''\n",
        "    Aggregate ticks to an OHLCV-like bar.\n",
        "\n",
        "    - open: first tick price in window\n",
        "    - high/low: extrema\n",
        "    - close: last tick price in window\n",
        "    - volume_qty: sum of qty\n",
        "\n",
        "    Assumes ticks are within [window_start_ms, window_end_ms) and same symbol.\n",
        "    '''\n",
        "    if not ticks:\n",
        "        raise ValueError(\"ticks must be non-empty\")\n",
        "\n",
        "    ticks_sorted = sorted(ticks, key=lambda t: t[\"ts_ms\"])\n",
        "    prices = [float(t[\"price\"]) for t in ticks_sorted]\n",
        "    qtys = [float(t[\"qty\"]) for t in ticks_sorted]\n",
        "\n",
        "    return {\n",
        "        \"symbol\": symbol,\n",
        "        \"window_start_ms\": int(window_start_ms),\n",
        "        \"window_end_ms\": int(window_end_ms),\n",
        "        \"open\": prices[0],\n",
        "        \"high\": max(prices),\n",
        "        \"low\": min(prices),\n",
        "        \"close\": prices[-1],\n",
        "        \"volume_qty\": float(sum(qtys)),\n",
        "        \"tick_count\": int(len(ticks_sorted)),\n",
        "    }\n",
        "\n",
        "\n",
        "def detect_jump(prev_price: float, price: float, threshold_pct: float = 0.5) -> bool:\n",
        "    '''\n",
        "    Flag a \"jump\" if absolute percent move exceeds threshold_pct.\n",
        "    Default: 0.5% between consecutive ticks (tune this for your feed granularity).\n",
        "    '''\n",
        "    if prev_price <= 0 or price <= 0:\n",
        "        return False\n",
        "    pct = abs(price / prev_price - 1.0) * 100.0\n",
        "    return pct >= threshold_pct\n",
        "\n",
        "\n",
        "run_tests()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kafka producer (live WebSocket \u2192 Kafka)\n",
        "\n",
        "This producer connects to an exchange WebSocket, normalizes each trade tick, and publishes to Kafka.\n",
        "\n",
        "Key Kafka producer choices:\n",
        "- **key = symbol** \u2192 ensures all events for a symbol land in the same partition (preserves ordering per symbol)\n",
        "- **acks='all'** (optional) \u2192 stronger durability\n",
        "- **linger_ms / batch_size** (optional) \u2192 improve throughput\n",
        "\n",
        "> If you can't reach the exchange from this notebook, run this producer as a standalone script on a machine that can. The consumer section below still works the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import websockets\n",
        "from kafka import KafkaProducer\n",
        "\n",
        "BINANCE_WS = \"wss://stream.binance.com:9443/ws\"\n",
        "\n",
        "def make_producer() -> KafkaProducer:\n",
        "    return KafkaProducer(\n",
        "        bootstrap_servers=KAFKA_BOOTSTRAP,\n",
        "        key_serializer=lambda k: k.encode(\"utf-8\"),\n",
        "        value_serializer=lambda v: json.dumps(v).encode(\"utf-8\"),\n",
        "        acks=\"all\",\n",
        "        retries=5,\n",
        "        linger_ms=20,\n",
        "    )\n",
        "\n",
        "def parse_binance_trade(msg: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    '''\n",
        "    Binance trade payload typically includes:\n",
        "      s = symbol\n",
        "      T = trade time (ms)\n",
        "      p = price (string)\n",
        "      q = quantity (string)\n",
        "    '''\n",
        "    tick = {\n",
        "        \"source\": \"binance\",\n",
        "        \"symbol\": msg.get(\"s\"),\n",
        "        \"ts_ms\": msg.get(\"T\"),\n",
        "        \"price\": float(msg.get(\"p\")),\n",
        "        \"qty\": float(msg.get(\"q\")),\n",
        "    }\n",
        "    return normalize_tick(tick)\n",
        "\n",
        "async def run_ws_producer(symbols: List[str], stop_after_seconds: Optional[int] = None) -> None:\n",
        "    '''\n",
        "    Connect to Binance WS and stream trades for the given symbols into Kafka.\n",
        "\n",
        "    stop_after_seconds: useful for demos/tests (None => run forever)\n",
        "    '''\n",
        "    producer = make_producer()\n",
        "    start = time.time()\n",
        "\n",
        "    try:\n",
        "        streams = \"/\".join([f\"{s.lower()}@trade\" for s in symbols])\n",
        "        url = f\"{BINANCE_WS}/{streams}\"\n",
        "        async with websockets.connect(url, ping_interval=20, ping_timeout=20) as ws:\n",
        "            print(f\"Connected to {url}\")\n",
        "            while True:\n",
        "                if stop_after_seconds is not None and (time.time() - start) >= stop_after_seconds:\n",
        "                    print(\"Stopping producer (time limit reached).\")\n",
        "                    break\n",
        "\n",
        "                raw = await ws.recv()\n",
        "                payload = json.loads(raw)\n",
        "                data = payload.get(\"data\", payload)\n",
        "\n",
        "                try:\n",
        "                    tick = parse_binance_trade(data)\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "                producer.send(TICKS_TOPIC, key=tick[\"symbol\"], value=tick)\n",
        "    finally:\n",
        "        producer.flush(5)\n",
        "        producer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the producer\n",
        "\n",
        "Uncomment to run live for ~60 seconds:\n",
        "\n",
        "```python\n",
        "asyncio.get_event_loop().run_until_complete(run_ws_producer(SYMBOLS, 60))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Example (disabled by default)\n",
        "# asyncio.get_event_loop().run_until_complete(run_ws_producer(SYMBOLS, stop_after_seconds=60))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kafka consumer (analytics)\n",
        "\n",
        "The consumer:\n",
        "- reads from `crypto.ticks`\n",
        "- maintains per-symbol in-memory state\n",
        "- forms fixed-time windows (e.g., 10s bars)\n",
        "- emits:\n",
        "  - bars dataframe (in notebook)\n",
        "  - optional anomaly events to `crypto.alerts`\n",
        "\n",
        "Operational notes:\n",
        "- **group_id** defines consumer group membership.\n",
        "- **auto_offset_reset** controls start position if no committed offsets exist.\n",
        "- To replay history, use a new `group_id` or seek manually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from kafka import KafkaConsumer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SymbolState:\n",
        "    prev_price: Optional[float] = None\n",
        "    window_start_ms: Optional[int] = None\n",
        "    ticks_in_window: Optional[List[Dict[str, Any]]] = None\n",
        "\n",
        "\n",
        "def make_consumer(group_id: str) -> KafkaConsumer:\n",
        "    return KafkaConsumer(\n",
        "        TICKS_TOPIC,\n",
        "        bootstrap_servers=KAFKA_BOOTSTRAP,\n",
        "        group_id=group_id,\n",
        "        key_deserializer=lambda k: k.decode(\"utf-8\") if k else None,\n",
        "        value_deserializer=lambda v: json.loads(v.decode(\"utf-8\")),\n",
        "        enable_auto_commit=True,\n",
        "        auto_offset_reset=\"latest\",  # change to \"earliest\" for replay\n",
        "        consumer_timeout_ms=1000,     # lets loops exit periodically\n",
        "        max_poll_records=1000,\n",
        "    )\n",
        "\n",
        "\n",
        "def floor_to_window_start(ts_ms: int, window_sec: int) -> int:\n",
        "    w = window_sec * 1000\n",
        "    return (ts_ms // w) * w\n",
        "\n",
        "\n",
        "def consume_and_aggregate(\n",
        "    run_seconds: int = 60,\n",
        "    window_sec: int = BAR_SECONDS,\n",
        "    jump_threshold_pct: float = 0.5,\n",
        ") -> pd.DataFrame:\n",
        "    '''\n",
        "    Consume ticks for run_seconds and return a DataFrame of bars.\n",
        "\n",
        "    This is notebook-friendly. In production, you'd run continuously,\n",
        "    checkpoint state, and write output to durable storage.\n",
        "    '''\n",
        "    consumer = make_consumer(group_id=f\"crypto-analytics-{int(time.time())}\")\n",
        "    producer_alerts = KafkaProducer(\n",
        "        bootstrap_servers=KAFKA_BOOTSTRAP,\n",
        "        key_serializer=lambda k: k.encode(\"utf-8\"),\n",
        "        value_serializer=lambda v: json.dumps(v).encode(\"utf-8\"),\n",
        "    )\n",
        "\n",
        "    states: Dict[str, SymbolState] = {}\n",
        "    bars: List[Dict[str, Any]] = []\n",
        "    start = time.time()\n",
        "\n",
        "    try:\n",
        "        while (time.time() - start) < run_seconds:\n",
        "            for msg in consumer:\n",
        "                tick_raw = msg.value\n",
        "                try:\n",
        "                    tick = normalize_tick(tick_raw)\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "                sym = tick[\"symbol\"]\n",
        "                st = states.setdefault(sym, SymbolState(prev_price=None, window_start_ms=None, ticks_in_window=[]))\n",
        "\n",
        "                if st.prev_price is not None and detect_jump(st.prev_price, tick[\"price\"], threshold_pct=jump_threshold_pct):\n",
        "                    alert = {\n",
        "                        \"type\": \"jump\",\n",
        "                        \"symbol\": sym,\n",
        "                        \"ts_ms\": tick[\"ts_ms\"],\n",
        "                        \"prev_price\": st.prev_price,\n",
        "                        \"price\": tick[\"price\"],\n",
        "                        \"threshold_pct\": jump_threshold_pct,\n",
        "                    }\n",
        "                    producer_alerts.send(ALERTS_TOPIC, key=sym, value=alert)\n",
        "\n",
        "                st.prev_price = tick[\"price\"]\n",
        "\n",
        "                ws = floor_to_window_start(tick[\"ts_ms\"], window_sec)\n",
        "                if st.window_start_ms is None:\n",
        "                    st.window_start_ms = ws\n",
        "\n",
        "                if ws != st.window_start_ms:\n",
        "                    we = st.window_start_ms + window_sec * 1000\n",
        "                    if st.ticks_in_window:\n",
        "                        bar = aggregate_to_bar(sym, st.window_start_ms, we, st.ticks_in_window)\n",
        "                        bars.append(bar)\n",
        "\n",
        "                    st.window_start_ms = ws\n",
        "                    st.ticks_in_window = []\n",
        "\n",
        "                st.ticks_in_window.append(tick)\n",
        "\n",
        "    finally:\n",
        "        consumer.close()\n",
        "        producer_alerts.flush(3)\n",
        "        producer_alerts.close()\n",
        "\n",
        "    df = pd.DataFrame(bars)\n",
        "    if not df.empty:\n",
        "        df[\"window_start\"] = pd.to_datetime(df[\"window_start_ms\"], unit=\"ms\", utc=True)\n",
        "        df = df.sort_values([\"symbol\", \"window_start\"]).reset_index(drop=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run analytics consumer\n",
        "\n",
        "This assumes the producer is sending ticks into Kafka already.\n",
        "\n",
        "```python\n",
        "df_bars = consume_and_aggregate(run_seconds=60, window_sec=10, jump_threshold_pct=0.5)\n",
        "df_bars.tail()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df_bars = consume_and_aggregate(run_seconds=20, window_sec=BAR_SECONDS, jump_threshold_pct=0.5)\n",
        "df_bars.tail(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization\n",
        "\n",
        "We'll plot:\n",
        "- BTC & ETH close prices per bar\n",
        "- BTC/ETH ratio if both exist\n",
        "\n",
        "If you have many bars, we cap to the most recent `MAX_POINTS_TO_PLOT`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def plot_bars(df: pd.DataFrame) -> None:\n",
        "    if df.empty:\n",
        "        print(\"No bars to plot (df is empty). Start the producer and try again.\")\n",
        "        return\n",
        "\n",
        "    d = df.sort_values(\"window_start\")\n",
        "    if len(d) > MAX_POINTS_TO_PLOT:\n",
        "        d = d.iloc[-MAX_POINTS_TO_PLOT:]\n",
        "\n",
        "    for sym in sorted(d[\"symbol\"].unique()):\n",
        "        ds = d[d[\"symbol\"] == sym]\n",
        "        plt.figure()\n",
        "        plt.plot(ds[\"window_start\"], ds[\"close\"])\n",
        "        plt.title(f\"{sym} close (per {BAR_SECONDS}s bar)\")\n",
        "        plt.xlabel(\"time (UTC)\")\n",
        "        plt.ylabel(\"price\")\n",
        "        plt.xticks(rotation=30)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    syms = set(d[\"symbol\"].unique())\n",
        "    if \"BTCUSDT\" in syms and \"ETHUSDT\" in syms:\n",
        "        btc = d[d[\"symbol\"] == \"BTCUSDT\"][[\"window_start\", \"close\"]].rename(columns={\"close\":\"btc\"})\n",
        "        eth = d[d[\"symbol\"] == \"ETHUSDT\"][[\"window_start\", \"close\"]].rename(columns={\"close\":\"eth\"})\n",
        "        m = pd.merge_asof(btc.sort_values(\"window_start\"), eth.sort_values(\"window_start\"), on=\"window_start\")\n",
        "        if not m.empty:\n",
        "            m[\"ratio\"] = (m[\"btc\"] / m[\"eth\"])\n",
        "            plt.figure()\n",
        "            plt.plot(m[\"window_start\"], m[\"ratio\"])\n",
        "            plt.title(\"BTC/ETH ratio (aligned bars)\")\n",
        "            plt.xlabel(\"time (UTC)\")\n",
        "            plt.ylabel(\"BTC / ETH\")\n",
        "            plt.xticks(rotation=30)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "plot_bars(df_bars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Persisting results (optional)\n",
        "\n",
        "Common patterns:\n",
        "- write bars to Parquet (local / S3 / GCS)\n",
        "- sink to a DB (Timescale/ClickHouse/Postgres)\n",
        "- Kafka Connect sinks for managed pipelines\n",
        "\n",
        "Below: write a local Parquet file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "OUT_DIR = Path(\"./out\")\n",
        "OUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "if not df_bars.empty:\n",
        "    out_path = OUT_DIR / \"crypto_bars.parquet\"\n",
        "    df_bars.to_parquet(out_path, index=False)\n",
        "    print(f\"Wrote: {out_path.resolve()}\")\n",
        "else:\n",
        "    print(\"No bars to write.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scaling and production notes\n",
        "\n",
        "1. **Partitioning**\n",
        "   - key by `symbol` to preserve ordering per symbol\n",
        "   - scale partitions as symbols / throughput increase\n",
        "\n",
        "2. **Delivery semantics**\n",
        "   - consumers are typically at-least-once\n",
        "   - de-duplicate by `(symbol, window_start_ms)` at sink if needed\n",
        "\n",
        "3. **State**\n",
        "   - notebook keeps state in memory\n",
        "   - production: Kafka Streams (RocksDB), Redis, or periodic checkpoints\n",
        "\n",
        "4. **Schema**\n",
        "   - JSON is fine for prototypes\n",
        "   - Avro/Protobuf + Schema Registry is better for evolution and governance"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}