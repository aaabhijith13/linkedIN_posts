{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaabhijith13/linkedIN_posts/blob/main/spark_streaming_realtime_analytics_detailed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee72d60c",
      "metadata": {
        "id": "ee72d60c"
      },
      "source": [
        "\n",
        "# Spark Streaming: Real-Time Analytics with Live API Data\n",
        "\n",
        "This notebook demonstrates **Spark Streaming using DStreams** to perform\n",
        "**near real-time analytics** on continuously arriving data.\n",
        "\n",
        "The goal is not just to ingest data, but to **extract real-time insights**\n",
        "such as trends, rolling averages, and anomaly signals.\n",
        "\n",
        "---\n",
        "\n",
        "## What This Notebook Covers\n",
        "\n",
        "- Live data ingestion from an external API\n",
        "- Micro-batch processing model\n",
        "- DStreams and RDD execution\n",
        "- Window-based analytics\n",
        "- Trend detection logic\n",
        "- Fault tolerance concepts\n",
        "- Practical limitations and best practices\n",
        "\n",
        "---\n",
        "\n",
        "## Use Case\n",
        "\n",
        "**Real-time cryptocurrency market analytics**:\n",
        "- Track live prices\n",
        "- Compute rolling averages\n",
        "- Detect short-term trends\n",
        "- Identify sudden price movements\n",
        "\n",
        "This pattern applies equally to:\n",
        "- Live transcripts\n",
        "- Sensor data\n",
        "- Clickstreams\n",
        "- Financial tick data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99034e37",
      "metadata": {
        "id": "99034e37"
      },
      "source": [
        "\n",
        "## 1. Spark Streaming Setup\n",
        "\n",
        "Spark Streaming operates on top of Spark using a **micro-batch model**.\n",
        "Each batch is processed as a standard Spark job.\n",
        "\n",
        "We create:\n",
        "- `SparkContext`\n",
        "- `StreamingContext`\n",
        "\n",
        "Batch interval: **10 seconds**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ce7b14a5-db7c-46a9-af7b-55092e4c4357",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce7b14a5-db7c-46a9-af7b-55092e4c4357",
        "outputId": "c85a99da-f7b5-44f2-848b-036e2ecbb549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3c1d4985-2e7f-4204-bbe3-416168ccefd7",
      "metadata": {
        "id": "3c1d4985-2e7f-4204-bbe3-416168ccefd7"
      },
      "outputs": [],
      "source": [
        "# Source - https://stackoverflow.com/a\n",
        "# Posted by Anup Ash, modified by community. See post 'Timeline' for change history\n",
        "# Retrieved 2026-01-13, License - CC BY-SA 3.0\n",
        "\n",
        "!export PYSPARK_SUBMIT_ARGS=\"--master local[2] pyspark-shell\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "59027964",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59027964",
        "outputId": "b5798ca0-8de4-49e1-f307-fbf3d9d42a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "sc = SparkContext(\"local[2]\", appName=\"SparkStreamingRealTimeAnalytics\")\n",
        "ssc = StreamingContext(sc, batchDuration=10)\n",
        "\n",
        "ssc.checkpoint(\"/tmp/spark_streaming_checkpoint\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab32f18d",
      "metadata": {
        "id": "ab32f18d"
      },
      "source": [
        "\n",
        "## 2. Why API-Based Streaming Needs Polling\n",
        "\n",
        "Spark Streaming does not directly consume REST APIs.\n",
        "Instead, we:\n",
        "1. Poll the API at fixed intervals\n",
        "2. Convert responses into RDDs\n",
        "3. Feed them into Spark using `queueStream`\n",
        "\n",
        "In production systems:\n",
        "- APIs â†’ Kafka â†’ Spark Streaming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4917e7d2",
      "metadata": {
        "id": "4917e7d2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import requests\n",
        "import time\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ee2934a",
      "metadata": {
        "id": "4ee2934a"
      },
      "source": [
        "\n",
        "## 3. Fetching Live Data from an External API\n",
        "\n",
        "We use the **CoinGecko API** to fetch live Bitcoin prices.\n",
        "Each API response is timestamped to support time-based analytics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "047d94b8",
      "metadata": {
        "id": "047d94b8"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "SESSION = requests.Session()\n",
        "SESSION.headers.update({\n",
        "    \"User-Agent\": \"spark-streaming-demo/1.0\"\n",
        "})\n",
        "\n",
        "def fetch_crypto_price():\n",
        "    url = \"https://api.coingecko.com/api/v3/simple/price\"\n",
        "    params = {\"ids\": \"bitcoin\", \"vs_currencies\": \"usd\"}\n",
        "\n",
        "    resp = SESSION.get(url, params=params, timeout=10)\n",
        "\n",
        "    # Fail fast on HTTP errors (429, 403, 5xx, etc.)\n",
        "    if resp.status_code != 200:\n",
        "        # Return None so streaming can continue\n",
        "        return None, f\"HTTP {resp.status_code}: {resp.text[:200]}\"\n",
        "\n",
        "    # Defensive JSON parsing\n",
        "    try:\n",
        "        payload = resp.json()\n",
        "    except Exception as e:\n",
        "        return None, f\"JSON decode error: {e}; body={resp.text[:200]}\"\n",
        "\n",
        "    # Validate expected shape\n",
        "    btc = payload.get(\"bitcoin\")\n",
        "    if not isinstance(btc, dict) or \"usd\" not in btc:\n",
        "        return None, f\"Unexpected payload: {str(payload)[:200]}\"\n",
        "\n",
        "    price = btc[\"usd\"]\n",
        "    ts = datetime.now(timezone.utc).isoformat()\n",
        "    return (ts, float(price)), None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c531b7",
      "metadata": {
        "id": "e8c531b7"
      },
      "source": [
        "\n",
        "## 4. Creating a DStream from Polled API Data\n",
        "\n",
        "Each API poll result becomes an RDD.\n",
        "Spark Streaming treats this as a continuous data stream.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "12d6d907",
      "metadata": {
        "id": "12d6d907"
      },
      "outputs": [],
      "source": [
        "\n",
        "rdd_queue = [sc.emptyRDD()]\n",
        "raw_stream = ssc.queueStream(rdd_queue, oneAtATime=True)\n",
        "\n",
        "price_stream = (\n",
        "    raw_stream\n",
        "    .filter(lambda x: x[0] is not None)   # keep successful records\n",
        "    .map(lambda x: x[0])                  # UNWRAP HERE\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cFu1OflldnJo"
      },
      "id": "cFu1OflldnJo"
    },
    {
      "cell_type": "markdown",
      "id": "de34b242",
      "metadata": {
        "id": "de34b242"
      },
      "source": [
        "\n",
        "## 5. Basic Transformations\n",
        "\n",
        "We extract the price value and prepare the data for analytics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "127ddc46",
      "metadata": {
        "id": "127ddc46"
      },
      "outputs": [],
      "source": [
        "# Extract price\n",
        "prices = price_stream.map(lambda x: x[1])\n",
        "\n",
        "# Pair for aggregation\n",
        "price_pairs = prices.map(lambda p: (\"BTC\", p))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a985b86e",
      "metadata": {
        "id": "a985b86e"
      },
      "source": [
        "\n",
        "## 6. Per-Batch Analytics\n",
        "\n",
        "Each micro-batch represents a near real-time snapshot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "011259d1",
      "metadata": {
        "id": "011259d1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Compute min, max, and average per batch\n",
        "batch_stats = price_pairs.groupByKey().mapValues(\n",
        "    lambda ps: {\n",
        "        \"min\": min(ps),\n",
        "        \"max\": max(ps),\n",
        "        \"avg\": sum(ps) / len(ps)\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cedbdab0",
      "metadata": {
        "id": "cedbdab0"
      },
      "source": [
        "\n",
        "## 7. Window-Based Analytics (Rolling Insights)\n",
        "\n",
        "Window operations allow analytics across **multiple batches**.\n",
        "\n",
        "Window configuration:\n",
        "- Window length: 60 seconds\n",
        "- Slide interval: 20 seconds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b7355ec3",
      "metadata": {
        "id": "b7355ec3"
      },
      "outputs": [],
      "source": [
        "\n",
        "windowed_avg = price_pairs.groupByKeyAndWindow(\n",
        "    windowDuration=60,\n",
        "    slideDuration=20\n",
        ").mapValues(lambda ps: sum(ps) / len(ps))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_stats.pprint()\n",
        "windowed_avg.pprint()"
      ],
      "metadata": {
        "id": "Uny3cr27cawe"
      },
      "id": "Uny3cr27cawe",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f4b61c29",
      "metadata": {
        "id": "f4b61c29"
      },
      "source": [
        "\n",
        "## 8. Trend Detection Logic\n",
        "\n",
        "We detect short-term trends by comparing:\n",
        "- Current window average\n",
        "- Previous window average\n",
        "\n",
        "This is a simplified trend signal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ace2f3be",
      "metadata": {
        "id": "ace2f3be"
      },
      "outputs": [],
      "source": [
        "\n",
        "def detect_trend(rdd):\n",
        "    data = rdd.collect()\n",
        "    if len(data) >= 2:\n",
        "        prev = data[-2][1]\n",
        "        curr = data[-1][1]\n",
        "        if curr > prev:\n",
        "            print(\"ðŸ“ˆ Upward trend detected\")\n",
        "        elif curr < prev:\n",
        "            print(\"ðŸ“‰ Downward trend detected\")\n",
        "        else:\n",
        "            print(\"âž¡ï¸ Stable trend\")\n",
        "\n",
        "windowed_avg.foreachRDD(detect_trend)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d586283a",
      "metadata": {
        "id": "d586283a"
      },
      "source": [
        "\n",
        "## 9. Fault Tolerance and Resilience\n",
        "\n",
        "Spark Streaming is fault tolerant because:\n",
        "- Each batch is an RDD\n",
        "- RDD lineage enables recomputation\n",
        "- Checkpointing preserves stateful operations\n",
        "\n",
        "If a worker node fails:\n",
        "- Lost partitions are recomputed automatically\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa3bd298",
      "metadata": {
        "id": "aa3bd298"
      },
      "source": [
        "\n",
        "## 10. Starting the Streaming Job\n",
        "\n",
        "The loop below continuously:\n",
        "- Polls the API\n",
        "- Feeds data into Spark\n",
        "- Triggers micro-batch execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7b9374ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b9374ca",
        "outputId": "0c22d7f7-823c-4f00-c6cb-c6ff0153f8fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(('2026-01-14T03:08:56.332727+00:00', 95300.0), None)\n",
            "-------------------------------------------\n",
            "Time: 2026-01-14 03:09:00\n",
            "-------------------------------------------\n",
            "\n",
            "(('2026-01-14T03:09:06.510957+00:00', 95300.0), None)\n",
            "-------------------------------------------\n",
            "Time: 2026-01-14 03:09:10\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2026-01-14 03:09:10\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2026-01-14 03:09:20\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2026-01-14 03:09:30\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2026-01-14 03:09:30\n",
            "-------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ssc.start()\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        record = fetch_crypto_price()\n",
        "        print(record)\n",
        "        if record:\n",
        "            rdd_queue.append(sc.parallelize([record]))\n",
        "        time.sleep(10)\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "\n",
        "ssc.stop(stopSparkContext=True, stopGraceFully=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c2b4db2",
      "metadata": {
        "id": "5c2b4db2"
      },
      "source": [
        "\n",
        "## Key Takeaways\n",
        "\n",
        "- Spark Streaming uses **micro-batches**\n",
        "- DStreams are sequences of RDDs\n",
        "- Window operations enable real-time analytics\n",
        "- In-memory processing reduces latency\n",
        "- Spark Streaming is **near real-time**, not event-by-event\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Kafka + Spark Streaming integration\n",
        "- Structured Streaming (modern replacement for DStreams)\n",
        "- Real-time ML inference pipelines\n",
        "- Stateful stream processing\n",
        "\n",
        "This notebook serves as a **foundation for real-world streaming analytics**.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}